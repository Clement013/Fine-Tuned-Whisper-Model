{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, WhisperForConditionalGeneration, WhisperTokenizer, WhisperProcessor\n",
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipe_model(model_id, language):\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "    processor = WhisperProcessor.from_pretrained(model_id)\n",
    "    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
    "    model.generation_config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    return pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        max_new_tokens=128,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58a1bf6dfd94def9e987cd6bd8677e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# openai/whisper-small\n",
    "# clt013/whisper-small-ft-malay-test-3\n",
    "# openai/whisper-large-v3\n",
    "# clt013/whisper-large-v3-ft-malay-test-1\n",
    "# Load the models\n",
    "whisper_small_model = get_pipe_model('openai/whisper-small','malay')\n",
    "after_ft_whisper_small_model = get_pipe_model('clt013/whisper-small-ft-malay-test-3','malay')\n",
    "whisper_large_model = get_pipe_model('openai/whisper-large-v3','malay')\n",
    "after_ft_whisper_large_model = get_pipe_model('clt013/whisper-large-v3-ft-malay-test-1','malay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\blocks.py:976: UserWarning: api_name predict already exists, using predict_1\n",
      "  warnings.warn(f\"api_name {api_name} already exists, using {api_name_}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "IMPORTANT: You are using gradio version 3.45.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "IMPORTANT: You are using gradio version 3.45.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "IMPORTANT: You are using gradio version 3.45.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\routes.py\", line 517, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\route_utils.py\", line 216, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\blocks.py\", line 1555, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\blocks.py\", line 1193, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\utils.py\", line 654, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\cleme\\AppData\\Local\\Temp\\ipykernel_46872\\3462615112.py\", line 34, in transcribe_file\n",
      "    return transcribe_microphone(audio_path, model_size)\n",
      "  File \"C:\\Users\\cleme\\AppData\\Local\\Temp\\ipykernel_46872\\3462615112.py\", line 28, in transcribe_microphone\n",
      "    original_text = transcriber.transcribe(audio_path, model_size, \"original\")\n",
      "  File \"C:\\Users\\cleme\\AppData\\Local\\Temp\\ipykernel_46872\\3462615112.py\", line 15, in transcribe\n",
      "    selected_model = self.models[model_size][model_type]\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleme\\AppData\\Local\\Temp\\gradio\\863f6717f8d54cc1aafb802405ad77623eba3445\\example_voice_1.wav\n",
      "C:\\Users\\cleme\\AppData\\Local\\Temp\\gradio\\863f6717f8d54cc1aafb802405ad77623eba3445\\example_voice_1.wav\n",
      "C:\\Users\\cleme\\AppData\\Local\\Temp\\gradio\\863f6717f8d54cc1aafb802405ad77623eba3445\\example_voice_1.wav\n",
      "C:\\Users\\cleme\\AppData\\Local\\Temp\\gradio\\863f6717f8d54cc1aafb802405ad77623eba3445\\example_voice_1.wav\n"
     ]
    }
   ],
   "source": [
    "class WhisperTranscriber:\n",
    "    def __init__(self, small_model, fine_tuned_small_model, large_model, fine_tuned_large_model):\n",
    "        self.models = {\n",
    "            \"small\": {\n",
    "                \"original\": small_model,\n",
    "                \"fine_tuned\": fine_tuned_small_model\n",
    "            },\n",
    "            \"large\": {\n",
    "                \"original\": large_model,\n",
    "                \"fine_tuned\": fine_tuned_large_model\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def transcribe(self, audio, model_size, model_type):\n",
    "        selected_model = self.models[model_size][model_type]\n",
    "        print(audio)\n",
    "        transcription = selected_model(audio)[\"text\"]\n",
    "        return transcription\n",
    "\n",
    "transcriber = WhisperTranscriber(\n",
    "    small_model=whisper_small_model,\n",
    "    fine_tuned_small_model=after_ft_whisper_small_model,\n",
    "    large_model=whisper_large_model,\n",
    "    fine_tuned_large_model=after_ft_whisper_large_model\n",
    ")\n",
    "\n",
    "def transcribe_microphone(audio_path, model_size):\n",
    "    original_text = transcriber.transcribe(audio_path, model_size, \"original\")\n",
    "    fine_tuned_text = transcriber.transcribe(audio_path, model_size, \"fine_tuned\")\n",
    "    return original_text, fine_tuned_text\n",
    "\n",
    "def transcribe_file(audio, model_size):\n",
    "    audio_path = audio.name\n",
    "    return transcribe_microphone(audio_path, model_size)\n",
    "    \n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>Whisper Small and Large Model Malay Fine-Tuned Demo</h1>\")\n",
    "    gr.Markdown(\"<p style='text-align: center; font-size: 18px;'> \\\n",
    "                    Realtime demo for Malay speech recognition using fine-tuned Whisper small and large models. \\\n",
    "                </p>\")\n",
    "    with gr.Tab(label=\"Microphone Input\"):\n",
    "        iface_mic = gr.Interface(\n",
    "            fn=transcribe_microphone,\n",
    "            inputs=[\n",
    "                gr.Microphone(type=\"filepath\"),\n",
    "                gr.Dropdown(choices=[\"small\", \"large\"], label=\"Model Size\"),\n",
    "            ],\n",
    "            outputs=[\n",
    "                gr.Textbox(label=\"Original Model Output\"),\n",
    "                gr.Textbox(label=\"Fine-Tuned Model Output\")\n",
    "            ],\n",
    "        )\n",
    "    with gr.Tab(label=\"File Upload Input\"):\n",
    "        iface_file = gr.Interface(\n",
    "            fn=transcribe_file,\n",
    "            inputs=[\n",
    "                gr.File(type=\"file\"),\n",
    "                gr.Dropdown(choices=[\"small\", \"large\"], label=\"Model Size\"),\n",
    "            ],\n",
    "            outputs=[\n",
    "                gr.Textbox(label=\"Original Model Output\"),\n",
    "                gr.Textbox(label=\"Fine-Tuned Model Output\")\n",
    "            ],\n",
    "        )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
